{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np  \n",
    "import random\n",
    "\n",
    "\n",
    "DEBUG_MODE = False\n",
    "\n",
    "json_file = \"/home/fpgadeveloper/Desktop/100/T20.json\"  # Input JSON file\n",
    "output_file = \"../Application/T20.json\"  # Output JSON file\n",
    "#output_file1 = \"c/TNC5000.json\"  # Output JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_application_model():\n",
    "    with open(json_file) as f:\n",
    "        app_model = json.load(f)\n",
    "    return app_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sender_receiver_pairs(AM):\n",
    "    sender_receiver_pair = {}\n",
    "    for sd_re in AM[\"application\"][\"messages\"]:\n",
    "        sender = sd_re[\"sender\"]\n",
    "        receiver = sd_re[\"receiver\"]\n",
    "        if sender not in sender_receiver_pair:\n",
    "            sender_receiver_pair[sender] = [receiver]\n",
    "        else:\n",
    "            sender_receiver_pair[sender].append(receiver)\n",
    "    return sender_receiver_pair\n",
    "\n",
    "\n",
    "\n",
    "# Function to find the sender receiver pairs as tuples , this for plotting the graph at later stages\n",
    "def find_sender_receiver_pairs_tuple(sender_receiver_pair):\n",
    "    vertex_edge_pairs = []\n",
    "\n",
    "    for source, targets in sender_receiver_pair.items():\n",
    "        for target in targets:\n",
    "            vertex_edge_pairs.append((source, target))\n",
    "    return vertex_edge_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(vertex_edge_pairs):\n",
    "    G = nx.DiGraph()\n",
    "    for v,e in vertex_edge_pairs:\n",
    "        G.add_edge(v,e)\n",
    "   \n",
    "    pos = nx.spring_layout(G, k=30)\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    if DEBUG_MODE:\n",
    "        nx.draw(G, with_labels=True, node_size=500, node_color='lightblue', font_size=10, font_color='black', arrowsize=10)\n",
    "        plt.show()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 20 nodes and 24 edges\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AM = read_application_model()\n",
    "sender_receiver_pair = find_sender_receiver_pairs(AM)\n",
    "vertex_edge_pairs = find_sender_receiver_pairs_tuple(sender_receiver_pair)\n",
    "G = plot_graph(vertex_edge_pairs)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting nodes:  [5, 11, 16, 18, 19]\n",
      "End nodes:  [0, 1, 2, 6, 13]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def finding_the_StartEnd_node(graph):\n",
    "    # Find starting nodes (nodes with no incoming edges)\n",
    "    starting_nodes = [node for node in graph.nodes() if graph.in_degree(node) == 0]\n",
    "\n",
    "    # Find end nodes (nodes with no outgoing edges)\n",
    "    end_nodes = [node for node in graph.nodes() if graph.out_degree(node) == 0]\n",
    "\n",
    "    return starting_nodes, end_nodes\n",
    "\n",
    "starting_nodes, end_nodes = finding_the_StartEnd_node(G)\n",
    "\n",
    "print(\"Starting nodes: \", starting_nodes)\n",
    "print(\"End nodes: \", end_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_run_on_starting_and_end_nodes = {1:{1},2:{2},3:{3},4:{4},5:{1,3,4},6:{3,4}}\n",
    "can_run_on_other_nodes = {1:{1},2:{1,2},3:{1,5},4:{1,6},5:{2},6:{1,5,6}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON: {'application': {'jobs': [{'id': 0, 'wcet_fullspeed': 88, 'mcet': 0, 'deadline': 10000, 'can_run_on': [2], 'processing_times': 10}, {'id': 1, 'wcet_fullspeed': 17, 'mcet': 0, 'deadline': 10000, 'can_run_on': [2], 'processing_times': 5}, {'id': 2, 'wcet_fullspeed': 45, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1], 'processing_times': 10}, {'id': 3, 'wcet_fullspeed': 8, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 6], 'processing_times': 5}, {'id': 4, 'wcet_fullspeed': 18, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1], 'processing_times': 1}, {'id': 5, 'wcet_fullspeed': 4, 'mcet': 0, 'deadline': 10000, 'can_run_on': [3], 'processing_times': 2}, {'id': 6, 'wcet_fullspeed': 16, 'mcet': 0, 'deadline': 10000, 'can_run_on': [4], 'processing_times': 2}, {'id': 7, 'wcet_fullspeed': 51, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 6], 'processing_times': 4}, {'id': 8, 'wcet_fullspeed': 82, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 6], 'processing_times': 10}, {'id': 9, 'wcet_fullspeed': 89, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 5, 6], 'processing_times': 6}, {'id': 10, 'wcet_fullspeed': 73, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 2], 'processing_times': 4}, {'id': 11, 'wcet_fullspeed': 48, 'mcet': 0, 'deadline': 10000, 'can_run_on': [3], 'processing_times': 8}, {'id': 12, 'wcet_fullspeed': 47, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 2], 'processing_times': 7}, {'id': 13, 'wcet_fullspeed': 55, 'mcet': 0, 'deadline': 10000, 'can_run_on': [4], 'processing_times': 10}, {'id': 14, 'wcet_fullspeed': 61, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 5, 6], 'processing_times': 4}, {'id': 15, 'wcet_fullspeed': 80, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1], 'processing_times': 3}, {'id': 16, 'wcet_fullspeed': 41, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 3, 4], 'processing_times': 5}, {'id': 17, 'wcet_fullspeed': 47, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 2], 'processing_times': 9}, {'id': 18, 'wcet_fullspeed': 81, 'mcet': 0, 'deadline': 10000, 'can_run_on': [1, 3, 4], 'processing_times': 10}, {'id': 19, 'wcet_fullspeed': 76, 'mcet': 0, 'deadline': 10000, 'can_run_on': [3, 4], 'processing_times': 4}], 'messages': [{'id': 0, 'sender': 3, 'receiver': 0, 'size': 24, 'timetriggered': True, 'period': 10}, {'id': 1, 'sender': 3, 'receiver': 1, 'size': 20, 'timetriggered': True, 'period': 30}, {'id': 2, 'sender': 3, 'receiver': 2, 'size': 15, 'timetriggered': True, 'period': 50}, {'id': 3, 'sender': 4, 'receiver': 1, 'size': 24, 'timetriggered': True, 'period': 10}, {'id': 4, 'sender': 5, 'receiver': 0, 'size': 21, 'timetriggered': True, 'period': 30}, {'id': 5, 'sender': 5, 'receiver': 9, 'size': 25, 'timetriggered': True, 'period': 50}, {'id': 6, 'sender': 7, 'receiver': 3, 'size': 23, 'timetriggered': True, 'period': 10}, {'id': 7, 'sender': 7, 'receiver': 4, 'size': 22, 'timetriggered': True, 'period': 30}, {'id': 8, 'sender': 8, 'receiver': 7, 'size': 25, 'timetriggered': True, 'period': 50}, {'id': 9, 'sender': 9, 'receiver': 8, 'size': 18, 'timetriggered': True, 'period': 10}, {'id': 10, 'sender': 10, 'receiver': 3, 'size': 22, 'timetriggered': True, 'period': 30}, {'id': 11, 'sender': 11, 'receiver': 6, 'size': 23, 'timetriggered': True, 'period': 50}, {'id': 12, 'sender': 11, 'receiver': 14, 'size': 24, 'timetriggered': True, 'period': 10}, {'id': 13, 'sender': 12, 'receiver': 8, 'size': 25, 'timetriggered': True, 'period': 30}, {'id': 14, 'sender': 14, 'receiver': 12, 'size': 16, 'timetriggered': True, 'period': 50}, {'id': 15, 'sender': 15, 'receiver': 8, 'size': 24, 'timetriggered': True, 'period': 10}, {'id': 16, 'sender': 15, 'receiver': 12, 'size': 21, 'timetriggered': True, 'period': 30}, {'id': 17, 'sender': 16, 'receiver': 15, 'size': 18, 'timetriggered': True, 'period': 50}, {'id': 18, 'sender': 17, 'receiver': 6, 'size': 23, 'timetriggered': True, 'period': 10}, {'id': 19, 'sender': 18, 'receiver': 3, 'size': 21, 'timetriggered': True, 'period': 30}, {'id': 20, 'sender': 18, 'receiver': 10, 'size': 23, 'timetriggered': True, 'period': 50}, {'id': 21, 'sender': 19, 'receiver': 7, 'size': 20, 'timetriggered': True, 'period': 10}, {'id': 22, 'sender': 19, 'receiver': 13, 'size': 21, 'timetriggered': True, 'period': 30}, {'id': 23, 'sender': 19, 'receiver': 17, 'size': 20, 'timetriggered': True, 'period': 50}]}, 'platform': {'nodes': [{'id': 0, 'is_router': True}, {'id': 1, 'is_router': False}, {'id': 2, 'is_router': False}, {'id': 3, 'is_router': True}, {'id': 4, 'is_router': False}, {'id': 5, 'is_router': False}, {'id': 6, 'is_router': True}, {'id': 7, 'is_router': False}, {'id': 8, 'is_router': False}], 'links': [{'start': 0, 'end': 1}, {'start': 0, 'end': 2}, {'start': 0, 'end': 3}, {'start': 3, 'end': 4}, {'start': 3, 'end': 5}, {'start': 3, 'end': 6}, {'start': 6, 'end': 7}, {'start': 6, 'end': 8}], 'frequencies': [500, 1000], 'schemes': [{'id': 0, 'wcdt': 0, 'wcct': 0, 'wccr': 1}]}}\n",
      "Updated JSON saved to ../Application/T20.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def update_can_run_on(json_data, stratnodes, Endnodes,can_run_on_starting_and_end_nodes,can_run_on_other_nodes ):\n",
    "    \"\"\"\n",
    "    Updates the `can_run_on` field in the JSON data to a specified list for all jobs.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): The original JSON data as a Python dictionary.\n",
    "        new_can_run_on (list): The new list of values for the `can_run_on` field.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated JSON data.\n",
    "    \"\"\"\n",
    "    for job in json_data.get(\"application\", {}).get(\"jobs\", []):\n",
    "        if job[\"id\"] in stratnodes or job[\"id\"] in Endnodes:    \n",
    "            job[\"can_run_on\"] = list(can_run_on_starting_and_end_nodes[random.choice(list(can_run_on_starting_and_end_nodes.keys()))])    \n",
    "        else:\n",
    "            job[\"can_run_on\"] = list(can_run_on_other_nodes[random.choice(list(can_run_on_other_nodes.keys()))] ) \n",
    "    return json_data\n",
    "\n",
    "def save_json_to_file(json_data, file_name):\n",
    "    \"\"\"\n",
    "    Saves the JSON data to a file.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): The JSON data to save.\n",
    "        file_name (str): The name of the file to save the data to.\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Load JSON data from the input file\n",
    "with open(json_file, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Update `can_run_on` field\n",
    "updated_data = update_can_run_on(AM, starting_nodes, end_nodes, can_run_on_starting_and_end_nodes,can_run_on_other_nodes)\n",
    "\n",
    "print(\"Updated JSON:\", updated_data)\n",
    "\n",
    "# # Save the updated JSON to the output file\n",
    "save_json_to_file(updated_data, output_file)\n",
    "\n",
    "print(f\"Updated JSON saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON saved to c/TNC5000.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### For Non Constrain Json File\n",
    "\n",
    "import json\n",
    "\n",
    "def update_can_run_on(json_data, new_can_run_on):\n",
    "    \"\"\"\n",
    "    Updates the `can_run_on` field in the JSON data to a specified list for all jobs.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): The original JSON data as a Python dictionary.\n",
    "        new_can_run_on (list): The new list of values for the `can_run_on` field.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated JSON data.\n",
    "    \"\"\"\n",
    "    for job in json_data.get(\"application\", {}).get(\"jobs\", []):\n",
    "        job[\"can_run_on\"] = new_can_run_on\n",
    "    return json_data\n",
    "\n",
    "def save_json_to_file(json_data, file_name):\n",
    "    \"\"\"\n",
    "    Saves the JSON data to a file.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): The JSON data to save.\n",
    "        file_name (str): The name of the file to save the data to.\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "\n",
    "# Load JSON data from the input file\n",
    "with open(json_file, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Update `can_run_on` field\n",
    "updated_data = update_can_run_on(data, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Save the updated JSON to the output file\n",
    "save_json_to_file(updated_data, output_file1)\n",
    "\n",
    "print(f\"Updated JSON saved to {output_file1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
