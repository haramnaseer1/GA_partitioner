# GA Partitioner: Thesis Guide

## Introduction
This document provides a simple overview of the GA Partitioner project, focusing on the Genetic Algorithm (GA), tensors, and the Graph Neural Network (GNN) model architecture. 

## Genetic Algorithm (GA) Overview

### How GA Works in This Project
1. **Initialization**: Start with a population of random task schedules (chromosomes). Each schedule assigns tasks to processors and sets start times.

2. **Fitness Evaluation**: For each schedule, calculate a fitness score based on:
   - Makespan (total completion time)
   - Constraint violations (precedence, processor eligibility)
   - Communication delays

3. **Selection**: Choose the best schedules (parents) to create the next generation.

4. **Crossover**: Combine parts of two parent schedules to create offspring.

5. **Mutation**: Randomly change small parts of schedules to introduce variety.

6. **Iteration**: Repeat steps 2-5 for many generations until convergence. The number of iterations (generations) is critical: **insufficient iterations can result in suboptimal solutions and constraint violations**. This is not a bug in the GA itself, but a limitation of the search process when not enough generations are run to fully explore the solution space. If we increase the number of iterations, the quality and validity of the solution generally improve, but this also increases the time required to generate the solution, which ultimately affects the overall project duration and computational cost.

7. **Result**: The best schedule found is the solution. If constraints are violated, it is usually due to the GA not having enough time (iterations) to find an optimal, valid solution.

### Key Benefits
- Handles complex constraints automatically
- Explores large solution spaces efficiently
- Provides valid schedules for task partitioning

### Important Note on Constraint Violations
Constraint violations in GA-generated schedules are typically caused by **insufficient iterations** during the optimization process. The GA algorithm itself is robust and does not have any inherent bugs; however, if the number of generations is too low, the search may not reach a fully optimal or valid solution. Increasing the number of iterations allows the GA to better satisfy all constraints and improve schedule quality.

## Tensors in Machine Learning

### What are Tensors?
Tensors are multi-dimensional arrays that store data. In machine learning, they're the building blocks for representing information like images, text, or graphs.

### Tensors in This Project
- **Node Features**: 3D tensors representing task properties (processing time, deadline, dependency count)
- **Edge Features**: 2D tensors for message sizes between tasks
- **Adjacency Matrix**: Sparse tensors showing task connections
- **Training Data**: Batches of graph tensors from GA-generated schedules

### Why Tensors?
- Efficient for GPU computation
- Handle variable-sized graphs
- Enable parallel processing of multiple schedules


## What is the GNN Model Doing?
The GNN (Graph Neural Network) model in this project is trained to predict valid task schedules directly from graph-structured input data. It takes the tensor representations of tasks and their dependencies (generated by the GA) and learns to assign each task to a processor, predict start times, durations, and communication delays, all while respecting constraints such as precedence and processor eligibility. The GNN generalizes from the training data to produce new, valid schedules for unseen applications, aiming to minimize makespan and constraint violations automatically.

## GNN Model Architecture

### What is a Graph Neural Network?
A GNN processes graph-structured data by passing information between connected nodes, learning patterns in relationships.

### Architecture Overview
The Constraint-Aware GNN consists of:

1. **Input Layer**:
   - Node features: [processing_time, deadline, num_dependencies]
   - Edge features: [message_size]

2. **Graph Convolutional Layers** (4 layers):
   - GATv2Conv with 8 attention heads
   - Hidden dimension: 256
   - Layer normalization and dropout

3. **Constraint-Aware Heads**:
   - **Processor Head**: Predicts eligible processor assignment (0-191)
   - **Start Time Head**: Predicts task start times
   - **Duration Head**: Predicts execution duration
   - **Communication Delay Head**: Models inter-task delays

4. **Loss Functions**:
   - Multi-task loss combining processor, timing, and constraint penalties
   - Eligibility masking ensures valid assignments
   - Precedence constraints enforced during training

### Key Features
- Learns constraints during training (no post-processing needed)
- Handles variable graph sizes
- Uses PyTorch Geometric for efficient graph operations

## Conclusion
This project combines evolutionary optimization (GA) with deep learning (GNN) to solve task scheduling problems. The GA generates training data as tensors, which the GNN learns to predict valid schedules directly. This approach ensures constraint satisfaction while leveraging neural networks' pattern recognition capabilities.

For implementation details, refer to the code files. For experimental results, check the training logs and validation reports.