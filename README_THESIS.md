# GA Partitioner: Thesis Guide

## Introduction
This document provides a simple overview of the GA Partitioner project, focusing on the Genetic Algorithm (GA), tensors, and the Graph Neural Network (GNN) model architecture. 

## Genetic Algorithm (GA) Overview

### How GA Works in This Project
1. **Initialization**: Start with a population of random task schedules (chromosomes). Each schedule assigns tasks to processors and sets start times.

2. **Fitness Evaluation**: For each schedule, calculate a fitness score based on:
   - Makespan (total completion time)
   - Constraint violations (precedence, processor eligibility)
   - Communication delays

3. **Selection**: Choose the best schedules (parents) to create the next generation.

4. **Crossover**: Combine parts of two parent schedules to create offspring.

5. **Mutation**: Randomly change small parts of schedules to introduce variety.

6. **Iteration**: Repeat steps 2-5 for many generations until convergence. The number of iterations (generations) is critical: **insufficient iterations can result in suboptimal solutions and constraint violations**. This is not a bug in the GA itself, but a limitation of the search process when not enough generations are run to fully explore the solution space. If we increase the number of iterations, the quality and validity of the solution generally improve, but this also increases the time required to generate the solution, which ultimately affects the overall project duration and computational cost.

7. **Result**: The best schedule found is the solution. If constraints are violated, it is usually due to the GA not having enough time (iterations) to find an optimal, valid solution.

### Key Benefits
- Handles complex constraints automatically
- Explores large solution spaces efficiently
- Provides valid schedules for task partitioning

### Important Note on Constraint Violations
Constraint violations in GA-generated schedules are typically caused by **insufficient iterations** during the optimization process. The GA algorithm itself is robust and does not have any inherent bugs; however, if the number of generations is too low, the search may not reach a fully optimal or valid solution. Increasing the number of iterations allows the GA to better satisfy all constraints and improve schedule quality.

## Tensors in Machine Learning

### What are Tensors?
Tensors are multi-dimensional arrays that store data. In machine learning, they're the building blocks for representing information like images, text, or graphs.

### Tensors in This Project
- **Node Features**: 3D tensors representing task properties (processing time, deadline, dependency count)
- **Edge Features**: 2D tensors for message sizes between tasks
- **Adjacency Matrix**: Sparse tensors showing task connections
- **Training Data**: Batches of graph tensors from GA-generated schedules

### Why Tensors?
- Efficient for GPU computation
- Handle variable-sized graphs
- Enable parallel processing of multiple schedules


## What is the GNN Model Doing?
The GNN (Graph Neural Network) model in this project is trained to predict valid task schedules directly from graph-structured input data. It takes the tensor representations of tasks and their dependencies (generated by the GA) and learns to assign each task to a processor, predict start times, durations, and communication delays, all while respecting constraints such as precedence and processor eligibility. The GNN generalizes from the training data to produce new, valid schedules for unseen applications, aiming to minimize makespan and constraint violations automatically.

## GNN Model Architecture

### What is a Graph Neural Network?
A GNN processes graph-structured data by passing information between connected nodes, learning patterns in relationships.

### Production-Optimized Platform-Aware GNN Architecture
The latest model integrates platform information, cross-attention, and physics-based scheduling logic:

1. **Input Layer**:
   - Node features: [processing_time, memory, io_intensity, ...]
   - Edge features: [data_volume]
   - Platform features: processor speeds, tiers, locations

2. **Graph Convolutional Layers** (GATv2Conv):
   - 3 layers, 4 attention heads, hidden dimension 192
   - Layer normalization and dropout

3. **Cross-Attention Module**:
   - Cross-attention between task and processor embeddings
   - Enables platform-aware scheduling

4. **Physics-Based Duration Head**:
   - Predicts duration using task processing time and expected processor speed
   - Ensures realistic execution times

5. **Constraint-Aware Heads**:
   - **Processor Assignment**: Softmax over eligible processors
   - **Start Time Prediction**: Precedence constraints enforced during forward pass
   - **Communication Delay Head**: Models tier-based and learned delays

6. **Loss Functions**:
   - Multi-task loss: processor assignment, makespan, precedence, overlap
   - Dynamic constraint annealing: penalties increase during training
   - Early stopping and checkpointing for robust training

7. **Normalization & Augmentation**:
   - Training data normalized for stable learning
   - Structure-preserving DAG augmentation for generalization

### Training Script Usage
The new training script supports automatic train/val split, normalization, and flexible configuration:

```bash
python train_gnn_constrained.py --training_data training_data.pt --epochs 150 --batch_size 16 --hidden_dim 192 --num_processors 192 --patience 20 --model_save_path model_best.pt
```

Key arguments:
- `--training_data`: Path to training data file
- `--val_split`: Validation split ratio
- `--epochs`: Number of training epochs
- `--batch_size`: Batch size
- `--hidden_dim`: Hidden dimension
- `--num_processors`: Number of processors
- `--patience`: Early stopping patience
- `--model_save_path`: Path to save best model

### Key Features
- Platform-aware scheduling (processor speeds, tiers, locations)
- Physics-based duration prediction
- Cross-attention for task-processor interaction
- Constraint annealing for robust learning
- Structure-preserving graph augmentation
- Efficient training and validation with PyTorch Geometric

## Conclusion
This project combines evolutionary optimization (GA) with a production-optimized, platform-aware GNN to solve complex task scheduling problems. The GA generates realistic training data as tensors, which the GNN learns to predict valid, efficient schedules directly. The new model architecture ensures constraint satisfaction, platform adaptation, and robust generalization, leveraging neural networks' pattern recognition and physics-based reasoning.

For implementation details, see `train_gnn_constrained.py`. For experimental results, check the training logs, validation reports, and saved model checkpoints.